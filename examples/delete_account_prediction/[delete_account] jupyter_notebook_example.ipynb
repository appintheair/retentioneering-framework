{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Goal__\n",
    "\n",
    "Our goal is to detect interface elements / screens of an app at which users' engagement drops significantly (some users tend to delete their app accounts).\n",
    "\n",
    "\n",
    "__Tasks__\n",
    "\n",
    "1. Collect data\n",
    "2. Prepare data \n",
    "3. Analyze data\n",
    "    1. Build pivot tables\n",
    "    2. Visualize users' trajectories in the app\n",
    "    3. Build the classifier\n",
    "        1. Classifier helps you to pick out specific users' trajectories\n",
    "        2. Classifier allows you to estimate the probability of user's account being deleted based on user's current trajectory. One can use this information to dynamically change the content of the app to prevent from that.\n",
    "\n",
    "__Expected results__\n",
    "\n",
    "1. One will identify the most \"problematic\" elements of an app\n",
    "2. One will get the classifier allowing to predict the account deletion based on current user's behaviour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Download data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import retentioneering\n",
    "import os\n",
    "from retentioneering.utils import download_events_multi, preparing\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client, job_config = retentioneering.init_from_file('./settings_yaml.yaml')\n",
    "settings = retentioneering.Config('./settings_yaml.yaml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### you can use one settings file in download_events_multi for multiple queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can see what kind of SQL query is transmitted to BQ\n",
    "#download_events_multi(client, job_config=job_config, settings=settings, return_only_query=True)[1];\n",
    "#download_events_multi(client, job_config=job_config, settings=settings, return_only_query=True)[0];"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = download_events_multi(client, job_config=job_config, settings=settings)\n",
    "print('Downloaded DataFrame shape: {}'.format(df.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create a new column 'group_name' with the name of the query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = '../../data' \n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/data_from_bq_delete_accounts.csv'\n",
    "df.to_csv(path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now you can prepare your DataFrame for further analysis using pandas instruments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop duplicated events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(path, sep=';', encoding='utf-8')\n",
    "df = preparing.drop_duplicated_events(df, settings=settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### drop out deleted group users from the test group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many users we have in each group\n",
    "df.groupby('group_name').user_pseudo_id.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split in two groups\n",
    "df_deleted = df.loc[df.group_name == u'deleted', :].copy()\n",
    "df_test = df.loc[df.group_name == u'test_group', :].copy()\n",
    "\n",
    "# select users from 'deleted' group\n",
    "selected_users = df_deleted.user_pseudo_id.unique()\n",
    "\n",
    "# drop selected users from second group\n",
    "df_test = df_test.loc[~df_test.user_pseudo_id.isin(selected_users)]\n",
    "\n",
    "print('deleted:', df_deleted.user_pseudo_id.nunique())\n",
    "print('test_group:', df_test.user_pseudo_id.nunique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### leave only users who delete their account in first two days and has 'first_open' event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave only users with first open\n",
    "has_first_open_users = df_deleted.loc[df_deleted.event_name == u'first_open', 'user_pseudo_id'].unique()\n",
    "df_deleted = df_deleted.loc[df_deleted.user_pseudo_id.isin(has_first_open_users), :]\n",
    "print('deleted group users:', df_deleted.user_pseudo_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for each user find timestamp when account was deleted, and timestamp of first event\n",
    "delete_account_time = df_deleted.groupby('user_pseudo_id', as_index=False)['event_timestamp'].max()\n",
    "first_event_time = df_deleted.groupby('user_pseudo_id', as_index=False)['event_timestamp'].min()\n",
    "\n",
    "# calculate users lifetime\n",
    "users_lifetime = first_event_time.merge(delete_account_time, \n",
    "                                        how='left', \n",
    "                                        on='user_pseudo_id', \n",
    "                                        suffixes=('_min', '_del'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "days_threshold = 2\n",
    "# select users with timedelta 'days_threshold' days and leave only them\n",
    "selected_users = users_lifetime[(users_lifetime['event_timestamp_del'] -\n",
    "                users_lifetime['event_timestamp_min']) / 1e6 / (24*60*60) <= days_threshold].user_pseudo_id.unique()\n",
    "\n",
    "df_deleted = df_deleted.loc[df_deleted.user_pseudo_id.isin(selected_users), :]\n",
    "print('deleted group users:', df_deleted.user_pseudo_id.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in second group leave only events within two days\n",
    "first_event_time = df_test.groupby('user_pseudo_id', as_index=False)['event_timestamp'].min()\n",
    "df_test = df_test.merge(first_event_time, how='left', on='user_pseudo_id', suffixes=('', '_min'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = df_test.loc[(df_test.event_timestamp - df_test.event_timestamp_min) / 1e6 / (24*60*60) <= days_threshold, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly select same size of users from test dataset\n",
    "np.random.seed(seed=42)\n",
    "df = df_test.loc[df_test.user_pseudo_id.isin(\n",
    "    np.random.choice(df_test.user_pseudo_id.unique(), \\\n",
    "                     size=df_deleted.user_pseudo_id.nunique(), \\\n",
    "                     replace=False)), :] \\\n",
    "                    .append(df_deleted, sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/data_from_bq_delete_accounts_prepared_data.csv'\n",
    "df.to_csv(path, sep=';', encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retentioneering import analysis, visualization\n",
    "from retentioneering.utils import preparing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../../data/data_from_bq_delete_accounts_prepared_data.csv'\n",
    "df = pd.read_csv(path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add accumulator events\n",
    "df = preparing.add_lost_events(\n",
    "    df, positive_event_name=u'settings_delete_account_success', negative_event_name=u'not_delete_account')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load filters for events\n",
    "# event_filter - common filters for the problem\n",
    "# additional_filter - additional filters for the task\n",
    "# lf - manualy added filters for the task\n",
    "\n",
    "event_filter = pd.read_csv('NewUserEventList.csv').values\n",
    "additional_filter = pd.read_csv('additional_filter.csv', ';') \n",
    "lf = {'app_provisional_enabledPush', 'app_enabledPush','app_enabledRemotePush'} \n",
    "additional_filter = additional_filter[additional_filter.Created_by == '1']['Event Action'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "event_filter2 = set(additional_filter)|set(event_filter[:, 0])|{'settings_delete_account_success', 'not_delete_account'}\n",
    "df = df[df.event_name.isin(event_filter2 - lf)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take only top-30 events for better visualization\n",
    "top_events = df.groupby('event_name').user_pseudo_id.count()\n",
    "top_events = set(top_events.sort_values().iloc[-20:].index)|set(['settings_delete_account_success', 'not_delete_account'])\n",
    "df = df[df.event_name.isin(top_events)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc = analysis.get_desc_table(df, target_event_list=['settings_delete_account_success',\n",
    "#                                                       'not_delete_account'],\n",
    "#                                max_steps=30, settings=settings, plot=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lost_users_list = df[df.event_name == 'settings_delete_account_success'].user_pseudo_id\n",
    "filt = df.user_pseudo_id.isin(lost_users_list)\n",
    "df_lost = df[filt]\n",
    "df_passed = df[~filt]\n",
    "\n",
    "desc_loss = analysis.get_desc_table(df_lost, target_event_list=['settings_delete_account_success',\n",
    "                                                                'not_delete_account'],\n",
    "                                    max_steps=30, settings=settings, plot=False)\n",
    "desc_passed = analysis.get_desc_table(df_passed, target_event_list=['settings_delete_account_success',\n",
    "                                                                    'not_delete_account'],\n",
    "                                      max_steps=30, settings=settings, plot=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "diff_df = analysis.get_diff(desc_loss, desc_passed, settings=settings, precalc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot graph with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_agg = analysis.get_all_agg(df_lost, agg_list=['trans_count'])\n",
    "visualization.plot.plot_graph(df_agg, 'trans_count', settings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or plot graph via api\n",
    "\n",
    "`It sends your data on our server`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from retentioneering.utils.export import plot_graph_api\n",
    "plot_graph_api(df_lost, settings, task='delete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model for prediction delete/non-delete ccount"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw data for model\n",
    "df = pd.read_csv(path, sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model\n",
    "event_filter_new = (set(additional_filter) | set(event_filter[:, 0]))\n",
    "#any( item == 'settings_delete_account' for item in event_filter_new)\n",
    "clf = analysis.Model(df, target_event='settings_delete_account_success', event_filter=event_filter_new,\n",
    "                     settings=settings)\n",
    "clf.fit_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can find leaky-events by analysing model output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = clf.model.coef_[0]\n",
    "names = clf._embedder.inverse_transform([importance])[0]\n",
    "importance = importance[importance != 0]\n",
    "tab = list(zip(names, importance))\n",
    "sorted(tab, key=lambda x: int(x[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We can see that `settings_delete_account` and `settings` has much more importance then other, so we should add it to filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# and rebuild model\n",
    "event_filter_new = (set(additional_filter)|set(event_filter[:, 0])).difference(set(['settings_delete_account', 'settings','profile_settings']))\n",
    "clf = analysis.Model(df, target_event='settings_delete_account_success',\n",
    "                     event_filter=event_filter_new,\n",
    "                     settings=settings)\n",
    "clf.fit_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat it\n",
    "importance = clf.model.coef_[0]\n",
    "names = clf._embedder.inverse_transform([importance])[0]\n",
    "importance = importance[importance != 0]\n",
    "list(zip(names, importance))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now all weights seem to be more uniformly distributed, so we can stop.\n",
    "\n",
    "We also can get most valued edges and nodes from model to visualize it on graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp_tracks = clf.build_important_track()\n",
    "# edges\n",
    "imp_tracks[imp_tracks[1].notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nodes\n",
    "imp_tracks[imp_tracks[1].isnull()][0].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
